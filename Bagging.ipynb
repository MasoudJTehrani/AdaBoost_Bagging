{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_HW3_Bagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dpy6xvja-Wb"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9VUGnYEbGkr"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib import style\n",
        "style.use('fivethirtyeight')\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import math\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXgXJ1sxbHlB"
      },
      "source": [
        "# importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a49u7iRcbN-A"
      },
      "source": [
        "Diabetes = pd.read_csv('Diabetes.txt', header=None, sep=\"\\t\")\n",
        "Diabetes = Diabetes.drop(9, axis=1)\n",
        "\n",
        "Ionosphere = pd.read_csv('Ionosphere.txt', header=None)\n",
        "\n",
        "Sonar = pd.read_csv('Sonar.txt', header=None)\n",
        "\n",
        "Breast = pd.read_csv('BreastTissue.txt', header=None, sep=\"\\t\")\n",
        "\n",
        "Glass = pd.read_csv('Glass.txt', header=None, sep=\"\\t\")\n",
        "Glass = Glass.drop(10, axis=1)\n",
        "\n",
        "Wine = pd.read_csv('Wine.txt', header=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0bBo_grcl6c"
      },
      "source": [
        "# Splitting the datasets into the Training set and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mSbfVOCctMQ",
        "outputId": "977124df-f72a-483f-8e06-1212089d039e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(Glass.iloc[:, :-1], Glass.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "\n",
        "train_Glass = pd.concat([X_train, y_train], axis =1)\n",
        "test_Glass = pd.concat([X_test, y_test], axis =1)\n",
        "\n",
        "print(train_Glass, '\\n')\n",
        "print(test_Glass)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           0      1     2     3      4     5      6     7     8  9\n",
            "8    1.51918  14.04  3.58  1.37  72.08  0.56   8.30  0.00  0.00  1\n",
            "103  1.52725  13.80  3.15  0.66  70.57  0.08  11.64  0.00  0.00  2\n",
            "77   1.51627  13.00  3.58  1.54  72.83  0.61   8.04  0.00  0.00  2\n",
            "123  1.51707  13.48  3.48  1.71  72.52  0.62   7.99  0.00  0.00  2\n",
            "128  1.52068  13.55  2.09  1.67  72.18  0.53   9.57  0.27  0.17  2\n",
            "..       ...    ...   ...   ...    ...   ...    ...   ...   ... ..\n",
            "56   1.51215  12.99  3.47  1.12  72.98  0.62   8.35  0.00  0.31  1\n",
            "94   1.51629  12.71  3.33  1.49  73.28  0.67   8.24  0.00  0.00  2\n",
            "101  1.51730  12.35  2.72  1.63  72.87  0.70   9.23  0.00  0.00  2\n",
            "146  1.51769  13.65  3.66  1.11  72.77  0.11   8.60  0.00  0.00  3\n",
            "35   1.51567  13.29  3.45  1.21  72.74  0.56   8.57  0.00  0.00  1\n",
            "\n",
            "[149 rows x 10 columns] \n",
            "\n",
            "           0      1     2     3      4     5      6     7     8  9\n",
            "110  1.52664  11.23  0.00  0.77  73.21  0.00  14.68  0.00  0.00  2\n",
            "29   1.51784  13.08  3.49  1.28  72.86  0.60   8.49  0.00  0.00  1\n",
            "81   1.51593  13.25  3.45  1.43  73.17  0.61   7.86  0.00  0.00  2\n",
            "95   1.51860  13.36  3.43  1.43  72.26  0.51   8.60  0.00  0.00  2\n",
            "209  1.51623  14.14  0.00  2.88  72.61  0.08   9.18  1.06  0.00  4\n",
            "..       ...    ...   ...   ...    ...   ...    ...   ...   ... ..\n",
            "19   1.51735  13.02  3.54  1.69  72.73  0.54   8.44  0.00  0.07  1\n",
            "32   1.51775  12.85  3.48  1.23  72.97  0.61   8.56  0.09  0.22  1\n",
            "1    1.51761  13.89  3.60  1.36  72.73  0.48   7.83  0.00  0.00  1\n",
            "114  1.51847  13.10  3.97  1.19  72.44  0.60   8.43  0.00  0.00  2\n",
            "10   1.51571  12.72  3.46  1.56  73.20  0.67   8.09  0.00  0.24  1\n",
            "\n",
            "[65 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ddGKpFjdQBe"
      },
      "source": [
        "# Testing our weak learner (weak decision tree)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q64ggWSedRwk",
        "outputId": "a21efd98-c555-414f-96c1-dcc9fd8ef66f"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Diabetes.iloc[:, :-1], Diabetes.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "weak_tree.fit(X_train, y_train)\n",
        "y_pred = weak_tree.predict(X_test)\n",
        "Diabetes_before = accuracy_score(y_test, y_pred) * 100\n",
        "print(\"%.2f%% For Diabetese\" %Diabetes_before)\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Ionosphere.iloc[:, :-1], Ionosphere.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "weak_tree.fit(X_train, y_train)\n",
        "y_pred = weak_tree.predict(X_test)\n",
        "Ionosphere_before = accuracy_score(y_test, y_pred) * 100\n",
        "print(\"%.2f%% For Ionosphere\" %Ionosphere_before)\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Sonar.iloc[:, :-1], Sonar.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "weak_tree.fit(X_train, y_train)\n",
        "y_pred = weak_tree.predict(X_test)\n",
        "Sonar_before = accuracy_score(y_test, y_pred) * 100\n",
        "print(\"%.2f%% For Sonar\" %Sonar_before)\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Wine.iloc[:, :-1], Wine.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "weak_tree.fit(X_train, y_train)\n",
        "y_pred = weak_tree.predict(X_test)\n",
        "Wine_before = accuracy_score(y_test, y_pred) * 100\n",
        "print(\"%.2f%% For Wine\" %Wine_before)\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=4)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Glass.iloc[:, :-1], Glass.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "weak_tree.fit(X_train, y_train)\n",
        "y_pred = weak_tree.predict(X_test)\n",
        "Glass_before = accuracy_score(y_test, y_pred) * 100\n",
        "print(\"%.2f%% For Glass\" %Glass_before)\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=4)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Breast.iloc[:, :-1], Breast.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "weak_tree.fit(X_train, y_train)\n",
        "y_pred = weak_tree.predict(X_test)\n",
        "Breast_before = accuracy_score(y_test, y_pred) * 100\n",
        "print(\"%.2f%% For Breast\" %Breast_before)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72.73% For Diabetese\n",
            "83.02% For Ionosphere\n",
            "77.78% For Sonar\n",
            "59.26% For Wine\n",
            "70.77% For Glass\n",
            "59.38% For Breast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EminJPZedsx"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwnmICsAegRv"
      },
      "source": [
        "def makeSamples(dataset,T):\n",
        "  samples = []\n",
        "  for i in range(T):\n",
        "    result = dataset.sample( frac = 1, replace = True)\n",
        "    samples.append(result)\n",
        "\n",
        "  return samples\n",
        "\n",
        "def addNoise(dataset,n):\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(dataset.iloc[:, :-1], dataset.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "  cols = [i for i in range(len(dataset.columns)-1)]\n",
        "  list_of_random_items = random.sample(cols, math.ceil((len(dataset.columns)-1) * n/100))\n",
        "  noise = X_train.iloc[:, list_of_random_items] + np.random.normal( 0,1, X_train.iloc[:, list_of_random_items].shape )\n",
        "  X_train.iloc[:,list_of_random_items] = noise\n",
        "\n",
        "  return X_train, X_test,y_train,y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zg0WfJpPVcP"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb8tSemTPbkI",
        "outputId": "a5a23dc3-0471-4efe-f674-454cbb3d9dcb"
      },
      "source": [
        "bagging_T = [11,21,31,41]\n",
        "\n",
        "#Diabetes\n",
        "print('\\t\\t  scores for Diabetes (%.2f%%' %Diabetes_before ,'before bagging)\\n' )\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "for T in bagging_T:\n",
        "  bagging_score = []\n",
        "  for iterate in range(10):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(Diabetes.iloc[:, :-1], Diabetes.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "    train_Diabetes = pd.concat([X_train, y_train], axis =1)\n",
        "    test_Diabetes = pd.concat([X_test, y_test], axis =1)\n",
        "    samples = makeSamples(train_Diabetes, T)\n",
        "    predictions = []\n",
        "    for i in range(T):   \n",
        "      model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "      predictions.append(model.predict(X_test))\n",
        "      \n",
        "    bagging = []\n",
        "    for c in range(len(predictions[0])):\n",
        "      col = [predictions[i][c] for i in range(T)]\n",
        "      bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "    bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "  \n",
        "  print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Ionosphere\n",
        "print('\\t\\t  scores for Ionosphere (%.2f%%' %Ionosphere_before ,'before bagging)\\n' )\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "for T in bagging_T:\n",
        "  bagging_score = []\n",
        "  for iterate in range(10):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(Ionosphere.iloc[:, :-1], Ionosphere.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "    train_Ionosphere = pd.concat([X_train, y_train], axis =1)\n",
        "    test_Ionosphere = pd.concat([X_test, y_test], axis =1)\n",
        "    samples = makeSamples(train_Ionosphere, T)\n",
        "    predictions = []\n",
        "    for i in range(T):   \n",
        "      model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "      predictions.append(model.predict(X_test))\n",
        "      \n",
        "    bagging = []\n",
        "    for c in range(len(predictions[0])):\n",
        "      col = [predictions[i][c] for i in range(T)]\n",
        "      bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "    bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "  \n",
        "  print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Sonar\n",
        "print('\\t\\t  scores for Sonar (%.2f%%' %Sonar_before ,'before bagging)\\n' )\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "for T in bagging_T:\n",
        "  bagging_score = []\n",
        "  for iterate in range(10):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(Sonar.iloc[:, :-1], Sonar.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "    train_Sonar = pd.concat([X_train, y_train], axis =1)\n",
        "    test_Sonar = pd.concat([X_test, y_test], axis =1)\n",
        "    samples = makeSamples(train_Sonar, T)\n",
        "    predictions = []\n",
        "    for i in range(T):   \n",
        "      model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "      predictions.append(model.predict(X_test))\n",
        "      \n",
        "    bagging = []\n",
        "    for c in range(len(predictions[0])):\n",
        "      col = [predictions[i][c] for i in range(T)]\n",
        "      bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "    bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "  \n",
        "  print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Wine\n",
        "print('\\t\\t  scores for Wine (%.2f%%' %Wine_before ,'before bagging)\\n' )\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "for T in bagging_T:\n",
        "  bagging_score = []\n",
        "  for iterate in range(10):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(Wine.iloc[:, :-1], Wine.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "    train_Wine = pd.concat([X_train, y_train], axis =1)\n",
        "    test_Wine = pd.concat([X_test, y_test], axis =1)\n",
        "    samples = makeSamples(train_Wine, T)\n",
        "    predictions = []\n",
        "    for i in range(T):   \n",
        "      model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "      predictions.append(model.predict(X_test))\n",
        "      \n",
        "    bagging = []\n",
        "    for c in range(len(predictions[0])):\n",
        "      col = [predictions[i][c] for i in range(T)]\n",
        "      bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "    bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "  \n",
        "  print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Glass\n",
        "print('\\t\\t  scores for Glass (%.2f%%' %Glass_before ,'before bagging)\\n' )\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=4)\n",
        "\n",
        "for T in bagging_T:\n",
        "  bagging_score = []\n",
        "  for iterate in range(10):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(Glass.iloc[:, :-1], Glass.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "    train_Glass = pd.concat([X_train, y_train], axis =1)\n",
        "    test_Glass = pd.concat([X_test, y_test], axis =1)\n",
        "    samples = makeSamples(train_Glass, T)\n",
        "    predictions = []\n",
        "    for i in range(T):   \n",
        "      model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "      predictions.append(model.predict(X_test))\n",
        "      \n",
        "    bagging = []\n",
        "    for c in range(len(predictions[0])):\n",
        "      col = [predictions[i][c] for i in range(T)]\n",
        "      bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "    bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "  \n",
        "  print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Breast\n",
        "print('\\t\\t  scores for Breast (%.2f%%' %Breast_before ,'before bagging)\\n' )\n",
        "\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=4)\n",
        "\n",
        "for T in bagging_T:\n",
        "  bagging_score = []\n",
        "  for iterate in range(10):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(Breast.iloc[:, :-1], Breast.iloc[:, -1], test_size = 0.3, shuffle=True)\n",
        "    train_Breast = pd.concat([X_train, y_train], axis =1)\n",
        "    test_Breast = pd.concat([X_test, y_test], axis =1)\n",
        "    samples = makeSamples(train_Breast, T)\n",
        "    predictions = []\n",
        "    for i in range(T):   \n",
        "      model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "      predictions.append(model.predict(X_test))\n",
        "      \n",
        "    bagging = []\n",
        "    for c in range(len(predictions[0])):\n",
        "      col = [predictions[i][c] for i in range(T)]\n",
        "      bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "    bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "  \n",
        "  print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t  scores for Diabetes (72.73% before bagging)\n",
            "\n",
            "\t\t\tbagging score for T = 11 : 72.03%\n",
            "\t\t\tbagging score for T = 21 : 71.77%\n",
            "\t\t\tbagging score for T = 31 : 74.03%\n",
            "\t\t\tbagging score for T = 41 : 72.03%\n",
            "------------------------------------------------------------------------------------------\n",
            "\t\t  scores for Ionosphere (83.02% before bagging)\n",
            "\n",
            "\t\t\tbagging score for T = 11 : 82.92%\n",
            "\t\t\tbagging score for T = 21 : 82.26%\n",
            "\t\t\tbagging score for T = 31 : 83.77%\n",
            "\t\t\tbagging score for T = 41 : 82.74%\n",
            "------------------------------------------------------------------------------------------\n",
            "\t\t  scores for Sonar (77.78% before bagging)\n",
            "\n",
            "\t\t\tbagging score for T = 11 : 70.95%\n",
            "\t\t\tbagging score for T = 21 : 75.08%\n",
            "\t\t\tbagging score for T = 31 : 74.92%\n",
            "\t\t\tbagging score for T = 41 : 73.02%\n",
            "------------------------------------------------------------------------------------------\n",
            "\t\t  scores for Wine (59.26% before bagging)\n",
            "\n",
            "\t\t\tbagging score for T = 11 : 67.41%\n",
            "\t\t\tbagging score for T = 21 : 72.96%\n",
            "\t\t\tbagging score for T = 31 : 66.30%\n",
            "\t\t\tbagging score for T = 41 : 70.93%\n",
            "------------------------------------------------------------------------------------------\n",
            "\t\t  scores for Glass (70.77% before bagging)\n",
            "\n",
            "\t\t\tbagging score for T = 11 : 68.62%\n",
            "\t\t\tbagging score for T = 21 : 71.23%\n",
            "\t\t\tbagging score for T = 31 : 68.15%\n",
            "\t\t\tbagging score for T = 41 : 68.31%\n",
            "------------------------------------------------------------------------------------------\n",
            "\t\t  scores for Breast (59.38% before bagging)\n",
            "\n",
            "\t\t\tbagging score for T = 11 : 65.00%\n",
            "\t\t\tbagging score for T = 21 : 66.25%\n",
            "\t\t\tbagging score for T = 31 : 67.19%\n",
            "\t\t\tbagging score for T = 41 : 68.75%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEUOCfAzuD3n"
      },
      "source": [
        "# Testing noisy data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC1jUwstbRQ1",
        "outputId": "6e66017a-2ef5-47ce-d984-da605a957dd5"
      },
      "source": [
        "Noises = [10,20,30]\n",
        "\n",
        "#Diabetes\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "\n",
        "for n in Noises:\n",
        "  X_train, X_test, y_train, y_test = addNoise(Diabetes,n)\n",
        "  weak_tree.fit(X_train, y_train)\n",
        "  predict = weak_tree.predict(X_test)\n",
        "  D_before_with_noise = accuracy_score(y_test, predict) * 100\n",
        "\n",
        "  print('\\n\\t\\t',n,'%% Noisy scores for Diabetes (%.2f%%' %D_before_with_noise ,'before bagging)' )\n",
        "  for T in bagging_T:\n",
        "    bagging_score = []\n",
        "    for iterate in range(10):\n",
        "      X_train, X_test, y_train, y_test = addNoise(Diabetes,n)\n",
        "      train_Diabetes = pd.concat([X_train, y_train], axis =1)\n",
        "      test_Diabetes = pd.concat([X_test, y_test], axis =1)\n",
        "\n",
        "      samples = makeSamples(train_Diabetes, T)\n",
        "      predictions = []\n",
        "      for i in range(T):   \n",
        "        model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "        predictions.append(model.predict(X_test))\n",
        "        \n",
        "      bagging = []\n",
        "      for c in range(len(predictions[0])):\n",
        "        col = [predictions[i][c] for i in range(T)]\n",
        "        bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "      bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "    \n",
        "    print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Ionosphere\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "for n in Noises:\n",
        "  X_train, X_test, y_train, y_test = addNoise(Ionosphere,n)\n",
        "  weak_tree.fit(X_train, y_train)\n",
        "  predict = weak_tree.predict(X_test)\n",
        "  I_before_with_noise = accuracy_score(y_test, predict) * 100\n",
        "  print('\\n\\t\\t',n,'%% Noisy scores for Ionosphere (%.2f%%' %I_before_with_noise ,'before bagging)' )\n",
        "  for T in bagging_T:\n",
        "    bagging_score = []\n",
        "    for iterate in range(10):\n",
        "      X_train, X_test, y_train, y_test = addNoise(Ionosphere,n) \n",
        "      train_Ionosphere = pd.concat([X_train, y_train], axis =1)\n",
        "      test_Ionosphere = pd.concat([X_test, y_test], axis =1)\n",
        "\n",
        "      samples = makeSamples(train_Ionosphere, T)\n",
        "      predictions = []\n",
        "      for i in range(T):   \n",
        "        model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "        predictions.append(model.predict(X_test))\n",
        "        \n",
        "      bagging = []\n",
        "      for c in range(len(predictions[0])):\n",
        "        col = [predictions[i][c] for i in range(T)]\n",
        "        bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "      bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "    \n",
        "    print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Sonar\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "for n in Noises:\n",
        "  X_train, X_test, y_train, y_test = addNoise(Sonar,n)\n",
        "  weak_tree.fit(X_train, y_train)\n",
        "  predict = weak_tree.predict(X_test)\n",
        "  S_before_with_noise = accuracy_score(y_test, predict) * 100\n",
        "  print('\\n\\t\\t',n,'%% Noisy scores for Sonar (%.2f%%' %S_before_with_noise ,'before bagging)' )\n",
        "  for T in bagging_T:\n",
        "    bagging_score = []\n",
        "    for iterate in range(10):\n",
        "      X_train, X_test, y_train, y_test = addNoise(Sonar,n)\n",
        "      train_Sonar = pd.concat([X_train, y_train], axis =1)\n",
        "      test_Sonar = pd.concat([X_test, y_test], axis =1)\n",
        "\n",
        "      samples = makeSamples(train_Sonar, T)\n",
        "      predictions = []\n",
        "      for i in range(T):   \n",
        "        model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "        predictions.append(model.predict(X_test))\n",
        "        \n",
        "      bagging = []\n",
        "      for c in range(len(predictions[0])):\n",
        "        col = [predictions[i][c] for i in range(T)]\n",
        "        bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "      bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "    \n",
        "    print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Wine\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n",
        "for n in Noises:\n",
        "  X_train, X_test, y_train, y_test = addNoise(Wine,n)\n",
        "  weak_tree.fit(X_train, y_train)\n",
        "  predict = weak_tree.predict(X_test)\n",
        "  W_before_with_noise = accuracy_score(y_test, predict) * 100\n",
        "  print('\\n\\t\\t',n,'%% Noisy scores for Wine (%.2f%%' %W_before_with_noise ,'before bagging)' )\n",
        "  for T in bagging_T:\n",
        "    bagging_score = []\n",
        "    for iterate in range(10):\n",
        "      X_train, X_test, y_train, y_test = addNoise(Wine,n)\n",
        "      train_Wine = pd.concat([X_train, y_train], axis =1)\n",
        "      test_Wine = pd.concat([X_test, y_test], axis =1)\n",
        "\n",
        "      samples = makeSamples(train_Wine, T)\n",
        "      predictions = []\n",
        "      for i in range(T):   \n",
        "        model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "        predictions.append(model.predict(X_test))\n",
        "        \n",
        "      bagging = []\n",
        "      for c in range(len(predictions[0])):\n",
        "        col = [predictions[i][c] for i in range(T)]\n",
        "        bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "      bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "    \n",
        "    print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Glass\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=4)\n",
        "for n in Noises:\n",
        "  X_train, X_test, y_train, y_test = addNoise(Glass,n)\n",
        "  weak_tree.fit(X_train, y_train)\n",
        "  predict = weak_tree.predict(X_test)\n",
        "  G_before_with_noise = accuracy_score(y_test, predict) * 100\n",
        "  print('\\n\\t\\t',n,'%% Noisy scores for Glass (%.2f%%' %G_before_with_noise ,'before bagging)' )\n",
        "  for T in bagging_T:\n",
        "    bagging_score = []\n",
        "    for iterate in range(10):\n",
        "      X_train, X_test, y_train, y_test = addNoise(Glass,n)\n",
        "      train_Glass = pd.concat([X_train, y_train], axis =1)\n",
        "      test_Glass = pd.concat([X_test, y_test], axis =1)\n",
        "\n",
        "      samples = makeSamples(train_Glass, T)\n",
        "      predictions = []\n",
        "      for i in range(T):   \n",
        "        model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "        predictions.append(model.predict(X_test))\n",
        "        \n",
        "      bagging = []\n",
        "      for c in range(len(predictions[0])):\n",
        "        col = [predictions[i][c] for i in range(T)]\n",
        "        bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "      bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "    \n",
        "    print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n",
        "\n",
        "print('-'*90)\n",
        "\n",
        "#Breast\n",
        "weak_tree = DecisionTreeClassifier(criterion=\"entropy\",max_depth=4)\n",
        "for n in Noises:\n",
        "  X_train, X_test, y_train, y_test = addNoise(Breast,n)\n",
        "  weak_tree.fit(X_train, y_train)\n",
        "  predict = weak_tree.predict(X_test)\n",
        "  B_before_with_noise = accuracy_score(y_test, predict) * 100\n",
        "  print('\\n\\t\\t',n,'%% Noisy scores for Breast (%.2f%%' %B_before_with_noise ,'before bagging)' )\n",
        "  for T in bagging_T:\n",
        "    bagging_score = []\n",
        "    for iterate in range(10):\n",
        "      X_train, X_test, y_train, y_test = addNoise(Breast,n)\n",
        "      train_Breast = pd.concat([X_train, y_train], axis =1)\n",
        "      test_Breast = pd.concat([X_test, y_test], axis =1)\n",
        "\n",
        "      samples = makeSamples(train_Breast, T)\n",
        "      predictions = []\n",
        "      for i in range(T):   \n",
        "        model = weak_tree.fit(samples[i].iloc[:, :-1], samples[i].iloc[:, -1])\n",
        "        predictions.append(model.predict(X_test))\n",
        "        \n",
        "      bagging = []\n",
        "      for c in range(len(predictions[0])):\n",
        "        col = [predictions[i][c] for i in range(T)]\n",
        "        bagging.append(np.unique(col)[np.argmax(np.unique(col, return_counts= True)[1])])  \n",
        "      bagging_score.append(accuracy_score(y_test, bagging) * 100)\n",
        "    \n",
        "    print('\\t\\t\\tbagging score for T =', T , ': %.2f%%' %np.mean(bagging_score))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\t\t 10 % Noisy scores for Diabetes (70.56% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 72.42%\n",
            "\t\t\tbagging score for T = 21 : 71.77%\n",
            "\t\t\tbagging score for T = 31 : 71.86%\n",
            "\t\t\tbagging score for T = 41 : 72.64%\n",
            "\n",
            "\t\t 20 % Noisy scores for Diabetes (70.56% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 72.03%\n",
            "\t\t\tbagging score for T = 21 : 70.17%\n",
            "\t\t\tbagging score for T = 31 : 72.51%\n",
            "\t\t\tbagging score for T = 41 : 72.38%\n",
            "\n",
            "\t\t 30 % Noisy scores for Diabetes (64.50% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 71.69%\n",
            "\t\t\tbagging score for T = 21 : 69.22%\n",
            "\t\t\tbagging score for T = 31 : 72.99%\n",
            "\t\t\tbagging score for T = 41 : 72.64%\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "\t\t 10 % Noisy scores for Ionosphere (82.08% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 81.89%\n",
            "\t\t\tbagging score for T = 21 : 80.85%\n",
            "\t\t\tbagging score for T = 31 : 83.11%\n",
            "\t\t\tbagging score for T = 41 : 81.13%\n",
            "\n",
            "\t\t 20 % Noisy scores for Ionosphere (78.30% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 81.23%\n",
            "\t\t\tbagging score for T = 21 : 80.94%\n",
            "\t\t\tbagging score for T = 31 : 83.77%\n",
            "\t\t\tbagging score for T = 41 : 82.92%\n",
            "\n",
            "\t\t 30 % Noisy scores for Ionosphere (72.64% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 82.83%\n",
            "\t\t\tbagging score for T = 21 : 82.26%\n",
            "\t\t\tbagging score for T = 31 : 81.70%\n",
            "\t\t\tbagging score for T = 41 : 81.42%\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "\t\t 10 % Noisy scores for Sonar (71.43% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 72.70%\n",
            "\t\t\tbagging score for T = 21 : 74.60%\n",
            "\t\t\tbagging score for T = 31 : 73.81%\n",
            "\t\t\tbagging score for T = 41 : 73.65%\n",
            "\n",
            "\t\t 20 % Noisy scores for Sonar (68.25% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 70.95%\n",
            "\t\t\tbagging score for T = 21 : 71.59%\n",
            "\t\t\tbagging score for T = 31 : 68.41%\n",
            "\t\t\tbagging score for T = 41 : 71.75%\n",
            "\n",
            "\t\t 30 % Noisy scores for Sonar (68.25% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 71.59%\n",
            "\t\t\tbagging score for T = 21 : 74.13%\n",
            "\t\t\tbagging score for T = 31 : 74.60%\n",
            "\t\t\tbagging score for T = 41 : 72.86%\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "\t\t 10 % Noisy scores for Wine (59.26% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 73.15%\n",
            "\t\t\tbagging score for T = 21 : 78.89%\n",
            "\t\t\tbagging score for T = 31 : 61.85%\n",
            "\t\t\tbagging score for T = 41 : 78.89%\n",
            "\n",
            "\t\t 20 % Noisy scores for Wine (55.56% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 68.70%\n",
            "\t\t\tbagging score for T = 21 : 69.44%\n",
            "\t\t\tbagging score for T = 31 : 65.56%\n",
            "\t\t\tbagging score for T = 41 : 65.74%\n",
            "\n",
            "\t\t 30 % Noisy scores for Wine (64.81% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 66.48%\n",
            "\t\t\tbagging score for T = 21 : 66.67%\n",
            "\t\t\tbagging score for T = 31 : 73.33%\n",
            "\t\t\tbagging score for T = 41 : 78.89%\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "\t\t 10 % Noisy scores for Glass (70.77% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 68.92%\n",
            "\t\t\tbagging score for T = 21 : 68.00%\n",
            "\t\t\tbagging score for T = 31 : 68.15%\n",
            "\t\t\tbagging score for T = 41 : 69.23%\n",
            "\n",
            "\t\t 20 % Noisy scores for Glass (58.46% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 64.15%\n",
            "\t\t\tbagging score for T = 21 : 66.77%\n",
            "\t\t\tbagging score for T = 31 : 69.23%\n",
            "\t\t\tbagging score for T = 41 : 67.08%\n",
            "\n",
            "\t\t 30 % Noisy scores for Glass (56.92% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 66.77%\n",
            "\t\t\tbagging score for T = 21 : 64.46%\n",
            "\t\t\tbagging score for T = 31 : 66.00%\n",
            "\t\t\tbagging score for T = 41 : 67.08%\n",
            "------------------------------------------------------------------------------------------\n",
            "\n",
            "\t\t 10 % Noisy scores for Breast (59.38% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 73.44%\n",
            "\t\t\tbagging score for T = 21 : 70.31%\n",
            "\t\t\tbagging score for T = 31 : 67.81%\n",
            "\t\t\tbagging score for T = 41 : 66.88%\n",
            "\n",
            "\t\t 20 % Noisy scores for Breast (59.38% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 69.69%\n",
            "\t\t\tbagging score for T = 21 : 60.62%\n",
            "\t\t\tbagging score for T = 31 : 65.00%\n",
            "\t\t\tbagging score for T = 41 : 69.06%\n",
            "\n",
            "\t\t 30 % Noisy scores for Breast (56.25% before bagging)\n",
            "\t\t\tbagging score for T = 11 : 69.38%\n",
            "\t\t\tbagging score for T = 21 : 65.31%\n",
            "\t\t\tbagging score for T = 31 : 65.62%\n",
            "\t\t\tbagging score for T = 41 : 68.75%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}